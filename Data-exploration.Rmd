---
title: "Data exploration"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE, echo=FALSE}
rm(list=ls()) # Clears workspace
knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo = TRUE)
#knitr::opts_knit$set(progress=FALSE)

# Packages
PKG <- c("tidyverse","knitr", "renv", "modelsummary", "kableExtra", "corrplot","RColorBrewer","lmtest","sandwich","pscl", "glmmTMB", "broom.mixed","vcd", "lmtest", "sf", "tmap", "tigris", "patchwork", "rnaturalearth", "rgeos", "ggrepel", "ggpubr", "margins", "spdep") 

for (p in PKG) {
  if(!require(p,character.only = TRUE)) {  
    install.packages(p)
    require(p,character.only = TRUE)}
}
rm(PKG,p)
# install.packages("countreg", repos="http://R-Forge.R-project.org")
# library(countreg)
renv::snapshot()
#renv::update()
```

## Analysis summary

This work estimates the effect of different sea-level rise adaptation strategies on adjacent recreation on the shoreline in California. This employs correlational regression methods, using geotagged social media data as a proxy for relative visitation to coastal areas alongside the primary adaptation strategy predictors and other relevant geospatial covariates. 

Traditionally engineered structures, as well as natural and nature-based features, can provide flood and erosion defense from coastal storms and sea level rise; however, a comprehensive understanding of the full suite of benefits of these strategies is necessary to understand the overall merits of different approaches. We present research investigating the differential effect of various shoreline protection strategies on recreation. Recreation is a vital contributor to coastal economies and social welfare in coastal communities that relies critically on the condition of the shoreline. 

Visitation is proxied via geotagged social media data from Flickr and Twitter. These are measured as average annual user days, from 2005 - 2017 for Flickr and 2012 - 2017 for Twitter. Values are extracted for cells within a hexagonal tessalation that intersect with the NOAA Environmental Sensitivity Index polyline for the California coastline. The user day counts are treated as a proxy for relative visitation and are the dependent variable in the visitation model.  

*Given that we have count data, and lots of zeros, natural choices for regression are a poisson, negative binomial, zero-inflated negative binomial, or a hurdle model. How to choose between these?*

Zero-inflated doesn't refer to lots of zeros in the outcome variable, but an outcome variable that you would expect to have lots of zeros that are based on a different process than generates non-zero values. As such, even with lots of zeros, a zero inflated negative binomial model may not be appropriate versus a standard NB model if the zeros originate from the same date generating process as the non-zero values. Hurdle models are generally used for sequential decision making and may not be appropriate here, as the decision process to visit the shoreline is typically one step.

Let's first explore some summary statistics. 

```{r data, include=FALSE}
# source("./CA-recreation-coastal-hazards.R") # Produces "lresults.csv" in next line, loads data needed for all scripts
df<-read_csv("lresults.csv") # Results of "CA-recreation-coastal-hazards.R"
df<-df[complete.cases(df),] # drops a few observations where the precipitation and air temp rasters returned NAs due to inadequate coverage
# rescaling population to thousands and distances to km, to deal with model convergence issues
df$sumpop<-df$sumpop/1000
df$rdist<-df$rdist/1000
df$wtlddist<-df$wtlddist/1000
```

<style>
  td {
    padding: 40px;
  }
</style>

```{r zeros, echo=FALSE, results='asis'}
df$hres<-as.factor(df$hres)
f1<-datasummary((ud == 0) + (ud > 0) ~ (N=1) + Percent(),
            data = df[df$hres==250&df$source=="PUD",],fmt="%.0f", title = 'Flickr Resolution 250m')
f2<-datasummary((ud == 0) + (ud > 0) ~ (N=1) + Percent(),
            data = df[df$hres==500&df$source=="PUD",],fmt="%.0f", title = 'Flickr Resolution 500m')
f3<-datasummary((ud == 0) + (ud > 0) ~ (N=1) + Percent(),
            data = df[df$hres==1000&df$source=="PUD",],fmt="%.0f", title = 'Flickr Resolution 1000m')

knitr::kables(list(f1,f2,f3)) %>%
  kable_styling(position = "center", bootstrap_options = "none")

t1<-datasummary((ud == 0) + (ud > 0) ~ (N=1) + Percent(),
            data = df[df$hres==250&df$source=="TUD",],fmt="%.0f", title = 'Twitter Resolution 250m')
t2<-datasummary((ud == 0) + (ud > 0) ~ (N=1) + Percent(),
            data = df[df$hres==500&df$source=="TUD",],fmt="%.0f", title = 'Twitter Resolution 500m')
t3<-datasummary((ud == 0) + (ud > 0) ~ (N=1) + Percent(),
            data = df[df$hres==1000&df$source=="TUD",],fmt="%.0f", title = 'Twitter Resolution 1000m')

knitr::kables(list(t1,t2,t3)) %>%
  kable_styling(position = "center", bootstrap_options = "none")

rm(f1,f2,f3,t1,t2,t3,summaryname)

```

The data is characterized by a significant number of zeros, that increase as a percentage of the data at higher resolutions. Twitter data seems to produce more zeros than flickr data at all resolutions. 

```{r continuous, echo=FALSE}
datasummary((`Distance to nearest road (km)`=rdist ) + (`Population within 8 miles`=sumpop) + (`Distance to nearest wetland (km)`=wtlddist) + (`Mean annual precipitation (mm/yr)`=meanprec) + (`Mean air temp (C)`=meanat) + (`Mean sea surface temp (C)`=meanSST) ~ Mean + Median + SD + Histogram, data = df[df$hres==250,],fmt="%.2f", title = 'Continuous Predictors 250m')

datasummary((`Distance to nearest road (km)`=rdist ) + (`Population within 8 miles`=sumpop) + (`Distance to nearest wetland (km)`=wtlddist) + (`Mean annual precipitation (mm/yr)`=meanprec) + (`Mean air temp (C)`=meanat) + (`Mean sea surface temp (C)`=meanSST) ~ Mean + Median + SD + Histogram, data = df[df$hres==1000,],fmt="%.2f", title = 'Continuous Predictors 1000m')
```

Summary statistics are shown for all potential continuous predictors. There is little variation across resolutions, but note that the higher resolution observation picks up finer scale variation in the 'distance to X' variables.

```{r discrete, echo=FALSE}
# Filtering area of influence of amenities represented in Yourcoast database
df<-mutate_at(df, vars(BIKE_PATH,BOATING,BT_FACIL_T,CAMPGROUND,DSABLDACSS,FEE,FISHING,PARKING,PTH_BEACH,RESTROOMS,STRS_BEACH), list(~ ifelse(YCdist>1000, 0, .)))

d1<-datasummary((`Bike path`=BIKE_PATH) + (`Boating`=BOATING) + (`Boating facilities`=BT_FACIL_T) + (`Campground`=CAMPGROUND) + (`Disabled Access`=DSABLDACSS) + (`Fee`=FEE) + (`Fishing`=FISHING) + (`Parking available`=PARKING) + (`Path to beach`=PTH_BEACH) + (`Restrooms`=RESTROOMS) + (`Stairs to beach`=STRS_BEACH) + (`SF Bay Water Trail`=WaterTrail) + (`SF Bay Trail`=BayTrail) ~ Mean + SD, data = df[df$hres==250,],fmt="%.2f", title = 'Binary Predictors 250m')
d2<-datasummary((`Bike path`=BIKE_PATH) + (`Boating`=BOATING) + (`Boating facilities`=BT_FACIL_T) + (`Campground`=CAMPGROUND) + (`Disabled Access`=DSABLDACSS) + (`Fee`=FEE) + (`Fishing`=FISHING) + (`Parking available`=PARKING) + (`Path to beach`=PTH_BEACH) + (`Restrooms`=RESTROOMS) + (`Stairs to beach`=STRS_BEACH) + (`SF Bay Water Trail`=WaterTrail) + (`SF Bay Trail`=BayTrail) ~ Mean + SD, data = df[df$hres==1000,],fmt="%.2f", title = 'Binary Predictors 1000m')

knitr::kables(list(d1,d2)) %>%
  kable_styling(position = "center", bootstrap_options = "none")

d3<-datasummary((`Exposed rocky shore`=x1a ) + (`Exposed solid man-made structures`=x1b) + (`Exposed wave-cut platforms in bedrock`=x2a) + (`Fine-to-medium grain sand beaches`=x3a) + (`Scarps and steep slopes in sand`=x3b) + (`Coarse-grained sand beaches`=x4) + (`Mixed sand and gravel beaches`=x5) + (`Gravel beaches`=x6a) + (`Riprap`=x6b) + (`Exposed tidal flats`=x7) + (`Sheltered rocky shores`=x8a) + (`Sheltered man-made structures`=x8b) + (`Sheltered riprap`=x8c) + (`Sheltered tidal flat`=x9a) + (`Vegetated low riverine banks`=x9b) + (`Salt and brackish marshes`=x10a) ~ Mean + SD, data = df[df$hres==250,],fmt="%.2f", title = 'ESI mean distance in meters @ 250m resolution')
d4<-datasummary((`Exposed rocky shore`=x1a ) + (`Exposed solid man-made structures`=x1b) + (`Exposed wave-cut platforms in bedrock`=x2a) + (`Fine-to-medium grain sand beaches`=x3a) + (`Scarps and steep slopes in sand`=x3b) + (`Coarse-grained sand beaches`=x4) + (`Mixed sand and gravel beaches`=x5) + (`Gravel beaches`=x6a) + (`Riprap`=x6b) + (`Exposed tidal flats`=x7) + (`Sheltered rocky shores`=x8a) + (`Sheltered man-made structures`=x8b) + (`Sheltered riprap`=x8c) + (`Sheltered tidal flat`=x9a) + (`Vegetated low riverine banks`=x9b) + (`Salt and brackish marshes`=x10a) ~ Mean + SD, data = df[df$hres==1000,],fmt="%.2f", title = 'ESI mean distance in meters @ 1000m resolution')

knitr::kables(list(d3,d4)) %>%
  kable_styling(position = "center", bootstrap_options = "none")

```

These values represent proportions of observations intersecting with ESI types, or within 1km of amenities in the [California Coastal Commission Yourcoast database](https://www.coastal.ca.gov/YourCoast/#/map). Amenities do not change markedly between resolutions as they are based on intersection with a buffer (within 1km of Yourcoast geotagged location). Most notable output from the summary of ESI lengths is that some types (sheltered tidal flats, marshes, etc.) tend to wind considerably leading to longer lateral length than other types. Also note that, when more than one type is present, all types are assigned the full length of the ESI segment, leading to aggregate ESI lengths that tend to exceed the resolution size. This effect is not just due to winding shorelines.  

```{r correlationESI, echo=FALSE, fig.align="center"}
cormat<-round(cor(df[,3:18]),2)
rownames(cormat)<-c("Exposed rocky shore (1a)","Exposed solid man-made structures (1b)","Exposed wave-cut platforms in bedrock (2a)","Fine-to-medium grain sand beaches (3a)","Scarps and steep slopes in sand (3b)","Coarse-grained sand beaches (4)","Mixed sand and gravel beaches (5)","Gravel beaches (6a)","Riprap (6b)","Exposed tidal flats (7)","Sheltered rocky shores (8a)","Sheltered man-made structures (8b)","Sheltered riprap (8c)","Sheltered tidal flat (9a)","Vegetated low riverine banks (9b)","Salt and brackish marshes (10a)")
colnames(cormat)<-c("1a","1b","2a","3a","3b","4","5","6a","6b","7","8a","8b","8c","9a","9b","10a")
corrplot(cormat, method = "color", type = "lower", col = brewer.pal(n=8, name = "RdBu"),tl.col = "black")
```

Correlogram of ESI types. Not much correlation, though tidal flats and marshes are often observed together.

```{r correlationYC, echo=FALSE, fig.align="center"}
cormat<- df[df$hres==1000,] %>%
  dplyr::select(sumpop,rdist,BIKE_PATH,BOATING,BT_FACIL_T,CAMPGROUND,DSABLDACSS,FEE,FISHING,PARKING,PTH_BEACH,RESTROOMS,STRS_BEACH,WaterTrail,BayTrail,wtlddist,meanprec,meanat,meanSST)
cormat<-round(cor(cormat),2)
# rownames(cormat)<-c("Exposed rocky shore (1a)","Exposed solid man-made structures (1b)","Exposed wave-cut platforms in bedrock (2a)","Fine-to-medium grain sand beaches (3a)","Scarps and steep slopes in sand (3b)","Coarse-grained sand beaches (4)","Mixed sand and gravel beaches (5)","Gravel beaches (6a)","Riprap (6b)","Exposed tidal flats (7)","Sheltered rocky shores (8a)","Sheltered man-made structures (8b)","Sheltered riprap (8c)","Sheltered tidal flat (9a)","Vegetated low riverine banks (9b)","Salt and brackish marshes (10a)")
# colnames(cormat)<-c("1a","1b","2a","3a","3b","4","5","6a","6b","7","8a","8b","8c","9a","9b","10a")
par(xpd = TRUE) # https://stackoverflow.com/questions/46331172/corrplot-margin-in-plot-window
corrplot(cormat, method = "color", type = "lower", col = brewer.pal(n=8, name = "RdBu"),tl.col = "black", tl.srt = 45, mar = c(2, 2, 2, 2))

```

Correlogram of other predictors. Relationships are stable across resolutions; the above plots correlations at 1000m resolution.

```{r `ESI crosswalk`, echo=FALSE}

df$RockyShore<-df$x1a + df$x2a + df$x6a +df$x8a
df$SandyBeach<-df$x3a + df$x3b + df$x4 + df$x5
df$Marshes<-df$x9a + df$x9b + df$x10a
df$Armored<-df$x1b + df$x6b + df$x8b + df$x8c

d1<-datasummary((`Rocky shore`=RockyShore) + (`Sandy beach`=SandyBeach) + (`Marshes`=Marshes) + (`Armored`=Armored) ~ Mean + SD, data = df[df$hres==1000,],fmt="%.2f", title = 'ESI Length (m) @ 1000m')
d2<-datasummary((`Rocky shore`=RockyShore) + (`Sandy beach`=SandyBeach) + (`Marshes`=Marshes) + (`Armored`=Armored) ~ Mean + SD, data = df[df$hres==500,],fmt="%.2f", title = 'ESI Length (m) @ 500m')
d3<-datasummary((`Rocky shore`=RockyShore) + (`Sandy beach`=SandyBeach) + (`Marshes`=Marshes) + (`Armored`=Armored) ~ Mean + SD, data = df[df$hres==250,],fmt="%.2f", title = 'ESI Length (m) @ 250m')

knitr::kables(list(d1,d2,d3)) %>%
  kable_styling(position = "center", bootstrap_options = "none")
```

A proposed crosswalk of ESI types to fewer shoreline classes: Rocky shore {1A, 2A, 6A, 8A}, Sandy beach {3A, 3B, 4, 5}, Marshes {9A, 9B, 10A}, Armored shore {1B, 6B, 8B, 8C}. 

```{r `Dep Var regressions`, echo=FALSE}
df$ihsud<-log(df$ud + sqrt(df$ud^2 + 1)) # Inverse hyperbolic sine transformation of user days
df$lnud<-log(df$ud+1) # Natural log (+1 for zeros) transformation of user days

## Do not transform count data DOI: 10.1111/j.2041-210X.2010.00021.x

regs<-function(rs,tipe,dv){ # tipe is either "PUD" or "TUD" / rs is one of {25, 500, 1000} / dv is either "ud" or "ihsud" or "lnud"
  models <- list()
  models.rob<-list()
  models[['Poisson']]<-glm(reformulate(termlabels = c("RockyShore","SandyBeach","Marshes","Armored","sumpop","rdist","BIKE_PATH","BOATING","BT_FACIL_T","CAMPGROUND","DSABLDACSS","FEE","FISHING","PARKING","PTH_BEACH","RESTROOMS","STRS_BEACH","WaterTrail","BayTrail","wtlddist","meanprec","meanat","meanSST"),response = dv), data = df[df$source==tipe&df$hres==rs,], family = poisson)
  models.rob[['Poisson']]<-coeftest(models[['Poisson']], vcov = sandwich)
  models[['Quasi-Poisson']]<-glm(reformulate(termlabels = c("RockyShore","SandyBeach","Marshes","Armored","sumpop","rdist","BIKE_PATH","BOATING","BT_FACIL_T","CAMPGROUND","DSABLDACSS","FEE","FISHING","PARKING","PTH_BEACH","RESTROOMS","STRS_BEACH","WaterTrail","BayTrail","wtlddist","meanprec","meanat","meanSST"),response = dv), data = df[df$source==tipe&df$hres==rs,], family = quasipoisson)
  models.rob[['Quasi-Poisson']]<-coeftest(models[['Quasi-Poisson']], vcov = sandwich)
  models[['Negative Binomial']]<-MASS::glm.nb(reformulate(termlabels = c("RockyShore","SandyBeach","Marshes","Armored","sumpop","rdist","BIKE_PATH","BOATING","BT_FACIL_T","CAMPGROUND","DSABLDACSS","FEE","FISHING","PARKING","PTH_BEACH","RESTROOMS","STRS_BEACH","WaterTrail","BayTrail","wtlddist","meanprec","meanat","meanSST"),response = dv), data = df[df$source==tipe&df$hres==rs,])
  models.rob[['Negative Binomial']]<-coeftest(models[['Negative Binomial']], vcov = sandwich)
  models[['Hurdle']]<-hurdle(reformulate(termlabels = c("RockyShore","SandyBeach","Marshes","Armored","sumpop","rdist","BIKE_PATH","BOATING","BT_FACIL_T","CAMPGROUND","DSABLDACSS","FEE","FISHING","PARKING","PTH_BEACH","RESTROOMS","STRS_BEACH","WaterTrail","BayTrail","wtlddist","meanprec","meanat","meanSST"),response = dv), data = df[df$source==tipe&df$hres==rs,], dist = "negbin")
  models.rob[['Hurdle']]<-coeftest(models[['Hurdle']], vcov = sandwich)
  models[['ZINB']]<-zeroinfl(reformulate(termlabels = c("RockyShore","SandyBeach","Marshes","Armored","sumpop","rdist","BIKE_PATH","BOATING","BT_FACIL_T","CAMPGROUND","DSABLDACSS","FEE","FISHING","PARKING","PTH_BEACH","RESTROOMS","STRS_BEACH","WaterTrail","BayTrail","wtlddist","meanprec","meanat","meanSST"),response = dv), data = df[df$source==tipe&df$hres==rs,], dist = "negbin")
  models.rob[['ZINB']]<-coeftest(models[['ZINB']], vcov = sandwich)

  return(list(models,models.rob))
}

PUD250<-regs(250,"PUD","ud")
PUD500<-regs(500,"PUD","ud")
PUD1000<-regs(1000,"PUD","ud")

TUD250<-regs(250,"TUD","ud")
TUD500<-regs(500,"TUD","ud")
TUD1000<-regs(1000,"TUD","ud")

# plot(PUD250[[1]]$Poisson, which=2, col=c("red")) # Q-Q plots
# plot(PUD250[[1]]$`Negative Binomial`, which=2, col=c("red"))
# 
# plot(TUD250[[1]]$Poisson, which=2, col=c("red"))
# plot(TUD250[[1]]$`Negative Binomial`, which=2, col=c("red"))

# Breusch-Pagan test for heteroskedasticity http://math.furman.edu/~dcs/courses/math47/R/library/lmtest/html/bptest.html
map(PUD1000[[1]],bptest)
map(PUD500[[1]],bptest)
map(PUD250[[1]],bptest)
map(TUD1000[[1]],bptest)
map(TUD500[[1]],bptest)
map(TUD250[[1]],bptest)
```

We have estimated models that treat this as a one (Poisson, Quasi-Poisson, Negative Binomial) or two-stage process (ZINB and hurdle).

From a theoretical perspective shoreline visitation choice is not typically treated as a two-stage process in random utility models of visitation. Furthermore, the choice to visit the beach versus do other activities does not seem like it should be well-modeled by the predictors we have here, despite the better fit. Finally, we do not know the home locations or identities of visitors, so we have no reliable way to reconstruct their broader choice set or recover other social/demographic variables.

```{r `glmmTMB implementation`, echo=FALSE}
# # Template model builder implementation of models
# invisible(glmmTMBControl(optCtrl=list(iter.max=1e4,eval.max=1e3))) 
# Pmodels <- list()
# Pmodels[['Poisson']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==250,], family = poisson)
# Pmodels[['Quasi-Poisson']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==250,], family = nbinom1)
# Pmodels[['Negative Binomial']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==250,], family = nbinom2)
# # Separating the model into two processes for the zeros and the non-zeros
# Pmodels[['Hurdle']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + rdist + sumpop + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==250,], family = "truncated_nbinom2", zi=~.)
# Pmodels[['Zero-inflated Negative Binomial']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==250,], zi = ~., family = nbinom2)
# 
# gm<-modelsummary::gof_map
# gm$omit<-ifelse(gm$raw == "nobs"|gm$raw == "df.residual",TRUE,FALSE)
# rows<-data.frame("term"="Num.Obs","Poisson"="27315","Quasi-Poisson"="27315","Negative Binomial"="27315")
# attr(rows,"position")<-49
# 
# msummary(Pmodels[1:3], stars = TRUE, title = "Different regression approaches, Flickr User Days 250m resolution", gof_map = gm, add_rows = rows)
# 
# # msummary(Pmodels[4], stars = TRUE, title = "Different regression approaches, Flickr User Days 250m resolution", gof_map = gm)
# # 
# # msummary(Pmodels[5], stars = TRUE, title = "Different regression approaches, Flickr User Days 250m resolution", gof_map = gm)
# summary(Pmodels[[4]])
# summary(Pmodels[[5]])

# No approach for robust standard errors in R for ZINB estimated by glmmTMB, if needed.
```

```{r `Rootograms`, echo=FALSE}
root<-function(x){ # x is a specific model in a list of outputs, i.e. PUD250[[1]]$Poisson 
  name<-deparse(substitute(x)) %>% # Transforming variable name to string 
  str_split("\\[\\[",simplify = TRUE) # Splitting string at left brackets
  name1<-name[1,1] # Extracting left string after split
  name2<-name[1,2] 
  name2<-str_split(name2,"\\$",simplify = TRUE)
  name2<-name2[1,2]
  name2<-gsub("`","",name2)
  
  bp<-cbind(df[df$source==str_sub(name1,1,3)&df$hres==noquote(str_sub(name1,4,7)),"ud"],(as.vector(predict(x,type = "response")))) # Uses string components to subset appropriate "source" and "hres" values
  colnames(bp)<-c("obs","pred")
  
  bp<-pivot_longer(bp, cols = c("obs","pred"), names_to = "status", values_to = "ud")
  bp$status<-as.factor(bp$status)
  bp$ud<-round(bp$ud)
  
  bp<-ggplot(bp) + 
  stat_bin(aes(x=ud, fill=status), position = position_dodge(preserve = "single"), breaks = seq(0, 25, by = 1)) +
  scale_x_continuous(limits = c(0,25)) +
  theme_minimal() +
  ggtitle(name2)
  
  return(bp)
}  

# Rootograms for various plots (can't lapply/map this https://stackoverflow.com/questions/64812690/use-object-name-in-function-with-map-lapply)
gg1000P<-list()
gg1000P[[1]]<-root(PUD1000[[1]]$Poisson)
gg1000P[[2]]<-root(PUD1000[[1]]$`Quasi-Poisson`)
gg1000P[[3]]<-root(PUD1000[[1]]$`Negative Binomial`)
gg1000P[[4]]<-root(PUD1000[[1]]$Hurdle)
gg1000P[[5]]<-root(PUD1000[[1]]$ZINB)

gg500P<-list()
gg500P[[1]]<-root(PUD500[[1]]$Poisson)
gg500P[[2]]<-root(PUD500[[1]]$`Quasi-Poisson`)
gg500P[[3]]<-root(PUD500[[1]]$`Negative Binomial`)
gg500P[[4]]<-root(PUD500[[1]]$Hurdle)
gg500P[[5]]<-root(PUD500[[1]]$ZINB)

gg250P<-list()
gg250P[[1]]<-root(PUD250[[1]]$Poisson)
gg250P[[2]]<-root(PUD250[[1]]$`Quasi-Poisson`)
gg250P[[3]]<-root(PUD250[[1]]$`Negative Binomial`)
gg250P[[4]]<-root(PUD250[[1]]$Hurdle)
gg250P[[5]]<-root(PUD250[[1]]$ZINB)

Proot<-wrap_elements(grid::textGrob('1000m')) + wrap_elements(grid::textGrob('500m')) + wrap_elements(grid::textGrob('250m')) +
  gg1000P[[1]] + gg500P[[1]] + gg250P[[1]] +
  gg1000P[[2]] + gg500P[[2]] + gg250P[[2]] +
  gg1000P[[3]] + gg500P[[3]] + gg250P[[3]] +
  gg1000P[[4]] + gg500P[[4]] + gg250P[[4]] +
  gg1000P[[5]] + gg500P[[5]] + gg250P[[5]] +
  plot_layout(ncol = 3, guides = 'collect') +
  plot_annotation(title = "Models for Flickr User Days")

gg1000T<-list()
gg1000T[[1]]<-root(TUD1000[[1]]$Poisson)
gg1000T[[2]]<-root(TUD1000[[1]]$`Quasi-Poisson`)
gg1000T[[3]]<-root(TUD1000[[1]]$`Negative Binomial`)
gg1000T[[4]]<-root(TUD1000[[1]]$Hurdle)
gg1000T[[5]]<-root(TUD1000[[1]]$ZINB)

gg500T<-list()
gg500T[[1]]<-root(TUD500[[1]]$Poisson)
gg500T[[2]]<-root(TUD500[[1]]$`Quasi-Poisson`)
gg500T[[3]]<-root(TUD500[[1]]$`Negative Binomial`)
gg500T[[4]]<-root(TUD500[[1]]$Hurdle)
gg500T[[5]]<-root(TUD500[[1]]$ZINB)

gg250T<-list()
gg250T[[1]]<-root(TUD250[[1]]$Poisson)
gg250T[[2]]<-root(TUD250[[1]]$`Quasi-Poisson`)
gg250T[[3]]<-root(TUD250[[1]]$`Negative Binomial`)
gg250T[[4]]<-root(TUD250[[1]]$Hurdle)
gg250T[[5]]<-root(TUD250[[1]]$ZINB)

Troot<-wrap_elements(grid::textGrob('1000m')) + wrap_elements(grid::textGrob('500m')) + wrap_elements(grid::textGrob('250m')) +
  gg1000T[[1]] + gg500T[[1]] + gg250T[[1]] +
  gg1000T[[2]] + gg500T[[2]] + gg250T[[2]] +
  gg1000T[[3]] + gg500T[[3]] + gg250T[[3]] +
  gg1000T[[4]] + gg500T[[4]] + gg250T[[4]] +
  gg1000T[[5]] + gg500T[[5]] + gg250T[[5]] +
  plot_layout(ncol = 3, guides = 'collect') +
  plot_annotation(title = "Models for Twitter User Days")

rm(gg1000P,gg1000T,gg500P,gg500T,gg250P,gg250T)
Proot
Troot
```

Rootograms are a useful way to compare model selection between these two, as they assess [overdispersion](https://www.theanalysisfactor.com/poisson-or-negative-binomial-using-count-model-diagnostics-to-select-a-model/), a key selection criteria between the models. Unfortunately, there is no ready package to summarize the results we have here, so we resort to side-by-side count plots.

```{r `Stata code`, echo=FALSE}
# write.csv(df,"results2.csv", row.names = FALSE)

# Code for ZINB model in Stata using results2.csv 

# import delimited C:\Users\XPSXIII\Downloads\results2.csv
# zinb ud rockyshore sandybeach marshes armored sumpop rdist bike_path boating bt_facil_t campground dsabldacss fee fishing parking pth_beach restrooms strs_beach watertrail baytrail wtlddist meanprec meanat meansst if source=="PUD"&hres==250, inflate(rockyshore sandybeach marshes armored sumpop rdist bike_path boating bt_facil_t campground dsabldacss fee fishing parking pth_beach restrooms strs_beach watertrail baytrail wtlddist meanprec meanat meansst) robust
```

```{r, `Model selection` echo=FALSE}
# Extract IC and log-likelihood values, make into a table
AICS<-data.frame(c("Poisson","Quasi-Poisson","Negative Binomial","Hurdle","ZINB"))
colnames(AICS)<-c("model")
AICS$PUD1000<-map(PUD1000[[1]],AIC)
AICS$PUD500<-map(PUD500[[1]],AIC)
AICS$PUD250<-map(PUD250[[1]],AIC)
AICS$TUD1000<-map(TUD1000[[1]],AIC)
AICS$TUD500<-map(TUD500[[1]],AIC)
AICS$TUD250<-map(TUD250[[1]],AIC)

LL<-data.frame(c("Poisson","Quasi-Poisson","Negative Binomial","Hurdle","ZINB"))
colnames(LL)<-c("model")
LL$PUD1000<-map(PUD1000[[1]],logLik)
LL$PUD500<-map(PUD500[[1]],logLik)
LL$PUD250<-map(PUD250[[1]],logLik)
LL$TUD1000<-map(TUD1000[[1]],logLik)
LL$TUD500<-map(TUD500[[1]],logLik)
LL$TUD250<-map(TUD250[[1]],logLik)
```

##Draft figures for paper

```{r `geospatial data`, echo=FALSE, include=FALSE}
PUD500m<-st_read("./Data/PUD_2005-2017_500m.gpkg")
PUD500m$PUD_YR_AVG<-PUD500m$PUD_YR_AVG*13 # Total PUD vs annual average
TUD500m<-st_read("./Data/TUD_2012-2017_500m.gpkg")
TUD500m$PUD_YR_AVG<-TUD500m$PUD_YR_AVG*6 # Total TUD vs annual average
PUD250m<-st_read("./Data/PUD_2005-2017_250m.gpkg")
PUD250m$PUD_YR_AVG<-PUD250m$PUD_YR_AVG*13 
TUD250m<-st_read("./Data/TUD_2012-2017_250m.gpkg")
TUD250m$PUD_YR_AVG<-TUD250m$PUD_YR_AVG*6 
PUD1km<-st_read("./Data/PUD_2005-2017_1000m.gpkg")
PUD1km$PUD_YR_AVG<-PUD1km$PUD_YR_AVG*13
TUD1km<-st_read("./Data/TUD_2012-2017_1000m.gpkg")
TUD1km$PUD_YR_AVG<-TUD1km$PUD_YR_AVG*6
```

```{r `spatial autocorrelation`, echo=FALSE}
# Global Moran's I, following https://rpubs.com/quarcs-lab/spatial-autocorrelation
nP1000<-poly2nb(PUD1km,queen = FALSE)
lnP1000<-nb2listw(nP1000)
gMP1000<-moran.test(PUD1km$PUD_YR_AVG, lnP1000) # MI stat 7.034348e-01, p-value < 2.2e-16

nP250<-poly2nb(PUD250m,queen = FALSE)
lnP250<-nb2listw(nP250)
gMP250<-moran.test(PUD250m$PUD_YR_AVG, lnP250) # MI stat 6.045783e-01, p-value < 2.2e-16 

nP500<-poly2nb(PUD500m,queen = FALSE) 
lnP500<-nb2listw(nP500)
gMP500<-moran.test(PUD500m$PUD_YR_AVG, lnP500) # MI stat 6.885574e-01, p-value < 2.2e-16 

nT1000<-poly2nb(TUD1km,queen = FALSE)
lnT1000<-nb2listw(nT1000)
gMT1000<-moran.test(TUD1km$PUD_YR_AVG, lnT1000) # MI stat 5.575148e-01, p-value < 2.2e-16 

nT250<-poly2nb(TUD250m,queen = FALSE) 
lnT250<-nb2listw(nT250)
gMT250<-moran.test(TUD250m$PUD_YR_AVG, lnT250) # MI stat 5.092229e-01, p-value < 2.2e-16 

nT500<-poly2nb(TUD500m,queen = FALSE)
lnT500<-nb2listw(nT500)
gMT500<-moran.test(TUD500m$PUD_YR_AVG, lnT500) # MI stat 5.911101e-01 , p-value < 2.2e-16 

```

```{r `map data/preparation`, echo=FALSE, include=FALSE}
# Guidance for tmaps package https://geocompr.robinlovelace.net/adv-map.html

Rds<-st_read("./Data/Roads_2015.gpkg") # Roads
ESI<-st_read("./Data/ESIL_CA.gpkg") # ESI
bbox<-filter(Rds, FULLNAME %in% c("Pilarcitos Ave")) # Bounding box

ESI<-st_transform(ESI, st_crs(PUD500m))
bbox<-st_transform(bbox, st_crs(ESI))
PUD500m<-st_join(PUD500m,ESI, left = FALSE)

TUD500m<-st_join(TUD500m,ESI, left = FALSE)

# PUD250m<-st_join(PUD250m,ESI, left = FALSE)
# 
# TUD250m<-st_join(TUD250m,ESI, left = FALSE)

# PUD1km<-st_join(PUD1km,ESI, left = FALSE)

# TUD1km<-st_join(TUD1km,ESI, left = FALSE)
```

```{r `regression visuals`, echo=FALSE}
## Table

# Model summary table - doesn't work on hurdle or zero-inflated models, but does work for robust standard errors estimated via sandwich/lmtest
msummary(PUD1000[[1]][1:3]) # Original standard error estimates
msummary(PUD1000[[2]][1:3]) # Sandwich standard error estimates
# # Coefficient plot - doesn't work on hurdle or zero-inflated models
# modelplot(PUD1000[[1]][1:3])


  a<-hurdle(reformulate(termlabels = c("RockyShore","SandyBeach","Marshes","Armored","sumpop","rdist","BIKE_PATH","BOATING","BT_FACIL_T","CAMPGROUND","DSABLDACSS","FEE","FISHING","PARKING","PTH_BEACH","RESTROOMS","STRS_BEACH","WaterTrail","BayTrail","wtlddist","meanprec","meanat","meanSST"),response = "ud"), data = df[df$source=="PUD"&df$hres==250,], dist = "negbin")
  
  b<-coeftest(a, vcov = sandwich(a)) 

  # coefci(a, vcov = sandwich)
  m0 <- hurdle(ud ~ 1, data = df[df$source=="PUD"&df$hres==250,], dist = "negbin")
  waldtest(m0, a, vcov = sandwich)
  
msummary(b)

## Figure
# Following https://socviz.co/modeling.html

mrgn_P<-margins(Pmodels[['Hurdle']], vcov = vcovHC(Pmodels[['Hurdle']])) # Huber-White Standard Error
mrgn_T<-margins(Tmodels[['Hurdle']])

summary(Pmodels[['Hurdle']])

coeftest(Tmodels[['Hurdle']], vcov = sandwich)


```



```{r `maps tmap`, echo=FALSE}
# Modifying the bounding box https://www.jla-data.net/eng/adjusting-bounding-box-of-a-tmap-map/
bbox_new<-st_bbox(bbox)
xrange <- bbox_new$xmax - bbox_new$xmin
yrange <- bbox_new$ymax - bbox_new$ymin
  bbox_new[1] <- bbox_new[1] - (8 * xrange) # xmin - left
  bbox_new[3] <- bbox_new[3] + (4 * xrange) # xmax - right
  bbox_new[2] <- bbox_new[2] - (6 * yrange) # ymin - bottom
  bbox_new[4] <- bbox_new[4] + (6 * yrange) # ymax - top
bbox_new <- bbox_new %>%  # take the bounding box ...
  st_as_sfc() # ... and make it a sf polygon

cdp<-places("CA", cb = TRUE) # Place names from US Census (tigris package)
cdp<-st_transform(cdp, st_crs(ESI))
rd.sc<-roads("CA","San Mateo") # Roads from US Census in San Mateo County, CA
rd.sc<-st_transform(rd.sc, st_crs(ESI))

breaks = c(0, 100, 200, 300, 400)
PUD500m.p<-tm_shape(rd.sc,bbox = bbox_new) +
  tm_lines(col="grey") +
  tm_shape(PUD500m) +
  tm_polygons(col = "PUD_YR_AVG", breaks = breaks, title = "Flickr User Days") + 
  tm_shape(ESI) +
  tm_lines(col="black") +
  tm_shape(cdp) +
  tm_text("NAME",size=0.75) +
  tm_add_legend("line", col = "black", labels = "ESI Line")

breaks = c(0, 200, 400, 600, 800)
TUD500m.p<-tm_shape(rd.sc,bbox = bbox_new) +
  tm_lines(col="grey") +
  tm_shape(TUD500m) +
  tm_polygons(col = "PUD_YR_AVG", breaks = breaks, title = "Twitter User Days") + 
  tm_shape(ESI) +
  tm_lines(col="black") +
  tm_shape(cdp) +
  tm_text("NAME",size=0.75) +
  tm_add_legend("line", col = "black", labels = "ESI Line")

plots500m<-tmap_arrange(PUD500m.p,TUD500m.p,ncol = 2)

#plots500m

# Modifying the bounding box https://www.jla-data.net/eng/adjusting-bounding-box-of-a-tmap-map/
bbox_new<-st_bbox(bbox)
xrange <- bbox_new$xmax - bbox_new$xmin
yrange <- bbox_new$ymax - bbox_new$ymin
  bbox_new[1] <- bbox_new[1] - (6 * xrange) # xmin - left
  bbox_new[3] <- bbox_new[3] + (1 * xrange) # xmax - right
  bbox_new[2] <- bbox_new[2] + (2 * yrange) # ymin - bottom
  bbox_new[4] <- bbox_new[4] + (2.75 * yrange) # ymax - top
bbox_new <- bbox_new %>%  # take the bounding box ...
  st_as_sfc() # ... and make it a sf polygon

ESI_bbox<-st_crop(ESI,bbox_new)
ESI_bbox<-separate(ESI_bbox,ESI,c("I1","I2","I3"),"/",convert = TRUE)

coler1<-brewer.pal(8,"Paired")
coler2<-coler1[2:4]

a<-tm_shape(rd.sc,bbox = bbox_new) +
  tm_lines(col="grey") +
  tm_shape(ESI_bbox) +
  tm_lines(col="I1", legend.col.show = TRUE, lwd = 4, palette = coler1) +
  tm_shape(cdp) +
  tm_text("NAME",size=0.75)

b<-tm_shape(rd.sc,bbox = bbox_new) +
  tm_lines(col="grey") +
  tm_shape(TUD500m) +
  tm_borders(col = "black") +
  tm_shape(ESI_bbox) +
  tm_lines(col="I1", legend.col.show = TRUE, title.col = "ESI Classes", lwd = 6, palette = coler1, labels = c("Exposed solid man-made structures", "Exposed wave-cut platforms in bedrock", "Fine-to-medium grain sand beaches", "Coarse-grained sand beaches", "Mixed sand and gravel beaches", "Riprap", "Sheltered man-made structures", "Sheltered riprap")) +
  tm_shape(ESI_bbox) +
  tm_lines(col="I2", legend.col.show = FALSE, lwd = 3, colorNA = NULL, palette = coler2) +
  tm_shape(cdp) +
  tm_text("NAME",size=0.75) 
  

c<-tmap_arrange(a,b,ncol = 1)


d<-tmap_arrange(PUD500m.p,TUD500m.p,b, nrow = 2, heights = c(1,1,1), widths = c(1,1,2))
d

```

```{r `maps ggplot`, echo=FALSE}
# Tmap_arrange multi map layout appears inadequate for a three pane figure
# Repeat of above code using ggplot2 and patchwork

bbox<-filter(Rds, FULLNAME %in% c("Pilarcitos Ave")) # Bounding box
bbox_new<-st_bbox(bbox)
xrange <- bbox_new$xmax - bbox_new$xmin
yrange <- bbox_new$ymax - bbox_new$ymin
  bbox_new[1] <- bbox_new[1] - (8 * xrange) # xmin - left
  bbox_new[3] <- bbox_new[3] + (4 * xrange) # xmax - right
  bbox_new[2] <- bbox_new[2] - (6 * yrange) # ymin - bottom
  bbox_new[4] <- bbox_new[4] + (6 * yrange) # ymax - top
bbox_new <- bbox_new %>%  # take the bounding box ...
  st_as_sfc() # ... and make it a sf polygon

cdp<-places("CA", cb = TRUE) # Place names from US Census (tigris package)
cdp<-st_transform(cdp, st_crs(Rds))
rd.sc<-roads("CA","San Mateo") # Roads from US Census in San Mateo County, CA
rd.sc<-st_transform(rd.sc, st_crs(Rds))
ESI<-st_transform(ESI, st_crs(Rds))
sm<-ne_download(scale = 10, type = "countries", returnclass = "sf")
sm<-st_transform(sm, st_crs(Rds))

## Flickr map
PUD500m$PUD_b<-cut(PUD500m$PUD_YR_AVG,
                      breaks = c(1, 100, 200, 300, 400, 500, 600), labels = c("1-100","101-200","201-300","301-400","401-500","501-600"))

cdp<-st_crop(cdp,bbox_new)
a<-ggplot() +
  geom_sf(data=sm, color = "tan") +
  geom_sf(data=rd.sc, color = "grey") +
  geom_sf(data = PUD500m, aes(fill=PUD_b)) +
  scale_fill_brewer(palette = "Oranges", na.translate = FALSE, guide = guide_legend(override.aes = list(linetype = "blank"), title = "Flickr User Days")) + # https://github.com/tidyverse/ggplot2/issues/2763
  geom_sf(data=ESI, aes(color="ESI Line"), show.legend = TRUE) +
  scale_color_manual(values = c("ESI Line" = "black"), name = "") +
  geom_label_repel(data = cdp, aes(label=NAME, geometry = geometry), stat = "sf_coordinates", min.segment.length = 0, size=2.5, point.padding = NA) +
  coord_sf(xlim=c(-122.5402,-122.3882),ylim=c(37.38338,37.5586), datum = NA) +
  theme_void() +
  theme(legend.position = c(0.25, 0.32), legend.margin = margin(-0.25,0,0,0, unit="cm"), panel.background = element_rect(fill = "aliceblue", color = NA), axis.title.x=element_blank(), axis.title.y=element_blank(), legend.text=element_text(size=7), legend.title=element_text(size=8), plot.margin=unit(c(0.1,0.1,0.1,0.1),"cm")) +
  xlab(NULL) + 
  ylab(NULL)

## Twitter map
TUD500m$PUD_b<-cut(TUD500m$PUD_YR_AVG,
                      breaks = c(1, 500, 1000, 1500, 2000, 2500, 3000), labels = c("1-500","501-1000","1001-1500","1501-2000","2001-2500","2501-3000"))

b<-ggplot() +
  geom_sf(data=sm, color = "tan") +
  geom_sf(data=rd.sc, color = "grey") +
  geom_sf(data = TUD500m, aes(fill=PUD_b)) +
  scale_fill_brewer(palette = "Oranges", na.translate = FALSE, guide = guide_legend(override.aes = list(linetype = "blank"), title = "Twitter User Days")) + # https://github.com/tidyverse/ggplot2/issues/2763
  geom_sf(data=ESI, color = "black") +
  geom_label_repel(data = cdp, aes(label=NAME, geometry = geometry), stat = "sf_coordinates", min.segment.length = 0, size=2.5, point.padding = NA) +
  coord_sf(xlim=c(-122.5402,-122.3882),ylim=c(37.38338,37.5586), datum = NA) +
  theme_void() +
  theme(legend.position = c(0.25, 0.27), panel.background = element_rect(fill = "aliceblue", color = NA), axis.title.x=element_blank(), axis.title.y=element_blank(), legend.text=element_text(size=7), legend.title=element_text(size=8), plot.margin=unit(c(0.1,0.1,0.1,0.1),"cm")) +
  xlab(NULL) + 
  ylab(NULL)

# Modifying the bounding box https://www.jla-data.net/eng/adjusting-bounding-box-of-a-tmap-map/
bbox_new<-st_bbox(bbox)
xrange <- bbox_new$xmax - bbox_new$xmin
yrange <- bbox_new$ymax - bbox_new$ymin
  bbox_new[1] <- bbox_new[1] - (6 * xrange) # xmin - left
  bbox_new[3] <- bbox_new[3] - (1 * xrange) # xmax - right
  bbox_new[2] <- bbox_new[2] + (2 * yrange) # ymin - bottom
  bbox_new[4] <- bbox_new[4] + (4 * yrange) # ymax - top
bbox_new <- bbox_new %>%  # take the bounding box ...
  st_as_sfc() # ... and make it a sf polygon

ESI_bbox<-separate(ESI,ESI,c("I1","I2","I3"),"/",convert = TRUE)
ESI_bbox$I1<-as.factor(ESI_bbox$I1)
ESI_bbox$I2<-as.factor(ESI_bbox$I2)
levels(ESI_bbox$I1)
ESI_bbox$I1<-fct_recode(ESI_bbox$I1,"Rocky shore"="1A","Solid man-made structures"="1B","Wave-cut platforms in bedrock"="2A","Scarps and steep slopes"="2B", "Fine-to-medium grain sand beaches"="3A", "Scarps and steep slopes"="3B","Coarse-grained sand beaches"="4","Mixed sand and gravel beaches"="5","Gravel beaches"="6A","Riprap"="6B","Boulder rubble"="6D","Tidal flats"="7","Rocky shore"="8A","Solid man-made structures"="8B","Riprap"="8C","Tidal flats"="9A","Vegetated low riverine banks"="9B","Tidal flats" = "9C","Salt and brackish marshes"="10A","Freshwater marshes"="10B","Swamps" = "10C","Scrub and shrub wetlands"="10D")
ESI_bbox$I2<-fct_recode(ESI_bbox$I2,"Rocky shore"="1A","Solid man-made structures"="1B","Wave-cut platforms in bedrock"="2A","Scarps and steep slopes"="2B", "Fine-to-medium grain sand beaches"="3A", "Scarps and steep slopes"="3B","Coarse-grained sand beaches"="4","Mixed sand and gravel beaches"="5","Gravel beaches"="6A","Riprap"="6B","Boulder rubble"="6D","Tidal flats"="7","Rocky shore"="8A","Solid man-made structures"="8B","Riprap"="8C","Tidal flats"="9A","Vegetated low riverine banks"="9B","Tidal flats" = "9C","Salt and brackish marshes"="10A","Freshwater marshes"="10B","Swamps" = "10C","Scrub and shrub wetlands"="10D")
coler1<-brewer.pal(12,"Paired")
coler2<-brewer.pal(12,"Set2")
coler1<-c(coler1,coler2)

ESI_bbox<-st_crop(ESI_bbox, xmin = -122.52, xmax = -122.38,
                                    ymin = 37.47, ymax = 37.52)
rd.sc_bbox<-st_crop(rd.sc, xmin = -122.52, xmax = -122.38,
                                    ymin = 37.47, ymax = 37.52)
sm_bbox<-st_crop(sm, xmin = -122.52, xmax = -122.38,
                                    ymin = 37.47, ymax = 37.52)
TUD500m_bbox<-st_crop(TUD500m, xmin = -122.52, xmax = -122.38,
                                    ymin = 37.47, ymax = 37.52)

c<-ggplot() +
  geom_sf(data=sm_bbox, color = "tan") +
  geom_sf(data=rd.sc_bbox, color = "grey") +
  geom_sf(data=TUD500m_bbox, color = "black") +
  geom_sf(data=ESI_bbox, aes(color=I1), lwd = 2.5, show.legend = FALSE) +
  scale_color_manual(values = coler1) +
  geom_sf(data=ESI_bbox, aes(color=I2), lwd = .75, show.legend = "line") +
  scale_color_manual(values = coler1, name = "NOAA ESI Classes", guide = guide_legend(override.aes = list(shape = NA, lwd = 2))) +
  coord_sf(xlim=c(-122.52,-122.38),ylim=c(37.47,37.52), datum = NA, expand = FALSE) + #bbox_new
  theme_void() +
  theme(panel.background = element_rect(fill = "aliceblue", color = NA), axis.title.x=element_blank(), axis.title.y=element_blank(), legend.text=element_text(size=7), legend.title=element_text(size=8), legend.position = c(0.76, 0.5), plot.margin=margin(0, 0, 0, 0, "cm"), legend.background = element_rect(fill="white", size=0.5, linetype="solid")) +
  xlab(NULL) + 
  ylab(NULL)

p<-c/(a+b) + theme(base_family = "mono")
ggsave("p.eps",p)

# Notes - if you used fixed width spatial plots as inputs into a multi-row patchwork plot, it appears that you are going to get some excessive white space between plots. Can edit this after in illustrator. Also, cropping the inputs prior to plotting removes a lot of excess vector content outside the coord_sf boundaries that is a pain to handle in illustrator.
```

```{r `scatterplot prep`, include=FALSE}
# Create inverse hyperbolic sine transformations of all visitation variables (use IHS to deal with lots of zeros that we don't want to throw away http://marcfbellemare.com/wordpress/12856#more-12856
# doi 10.1111/obes.12325
ihs <- function(x) { # Define inverse hyperbolic sine function
  y <- log(x + sqrt(x^2 + 1))
  return(y)
}

df$source<-as.factor(df$source)

# Long to wide for scatter plot in ggplot2
df1<-pivot_wider(data = df, id_cols = c(id,hres), names_from = source, values_from = ud)
df1$ihsPUD<-ihs(df1$PUD)
df1$ihsTUD<-ihs(df1$TUD)

# Plot
# ggplot(df1, aes(x=PUD,y=TUD, color=hres))+
#   geom_point() +
#   geom_smooth(method = lm) 
# 
# ggplot(df1[df1$hres==1000,], aes(x=PUD,y=TUD))+
#   geom_point() +
#   geom_smooth(method = lm)
# 
# ggplot(df1, aes(x=ihsPUD,y=ihsTUD, color=hres))+
#   geom_point() +
#   geom_smooth(method = lm) 

ggplot(df1[df1$hres==1000,], aes(x=ihsPUD,y=ihsTUD))+
  geom_point() +
  geom_smooth(method = lm) +
  theme_classic()

# Statistical comparison of IHS vs raw data
summary(md<-lm(TUD ~ PUD, data = df1[df1$hres==1000,])) # Untransformed
# https://rpubs.com/iabrady/residual-analysis
plot(md, which=1, col=c("blue")) # Residual plot
plot(md, which=2, col=c("red")) # Q-Q plot
# shapiro.test(md[['residuals']]) # can't use for samples > 5000
plot(md, which=3, col=c("blue"))  # Scale-Location Plot
plot(md, which=5, col=c("blue"))  # Residuals vs Leverage
nobs(lm(TUD ~ PUD, data = df1[df1$hres==1000,]))

summary(md<-lm(ihsTUD ~ ihsPUD, data = df1[df1$hres==1000,])) # IHS transformed
# https://rpubs.com/iabrady/residual-analysis
plot(md, which=1, col=c("blue")) # Residual plot
plot(md, which=2, col=c("red")) # Q-Q plot
#shapiro.test(md[['residuals']]) # can't use for samples > 5000
plot(md, which=3, col=c("blue"))  # Scale-Location Plot
plot(md, which=5, col=c("blue"))  # Residuals vs Leverage
nobs(lm(TUD ~ PUD, data = df1[df1$hres==1000,]))

# QQ plot looks much closer to normal with the IHS transformation

source("./SurveyComp.R")

```

```{r `scatterplot`, echo=FALSE}

ihsPTplot<-ggplot(df1[df1$hres==1000,], aes(x=ihsPUD,y=ihsTUD))+
  geom_point() +
  labs(x="IHS(Twitter User Days)",y="IHS(Photo User Days)") +
  geom_smooth(method = lm, color = "blue") +
  theme_classic() +
  stat_cor(aes(label = ..rr.label..), color = "blue", geom = "label")

ihsPTplot + surveycomp1

```

