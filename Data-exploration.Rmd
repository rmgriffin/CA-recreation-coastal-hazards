---
title: "Data exploration"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE, echo=FALSE}
rm(list=ls()) # Clears workspace
knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo = TRUE)
#knitr::opts_knit$set(progress=FALSE)

# Packages
PKG <- c("tidyverse","knitr", "renv", "modelsummary", "kableExtra", "corrplot","RColorBrewer","lmtest","sandwich","pscl", "glmmTMB", "broom.mixed","vcd", "lmtest", "sf", "tmap", "tigris") 

for (p in PKG) {
  if(!require(p,character.only = TRUE)) {  
    install.packages(p)
    require(p,character.only = TRUE)}
}
rm(PKG,p)
# install.packages("countreg", repos="http://R-Forge.R-project.org")
# library(countreg)
renv::snapshot()
```

## Analysis summary

This work estimates the effect of different sea-level rise adaptation strategies on adjacent recreation on the shoreline in California. This employs correlational regression methods, using geotagged social media data as a proxy for relative visitation to coastal areas alongside the primary adaptation strategy predictors and other relevant geospatial covariates. 

Traditionally engineered structures, as well as natural and nature-based features, can provide flood and erosion defense from coastal storms and sea level rise; however, a comprehensive understanding of the full suite of benefits of these strategies is necessary to understand the overall merits of different approaches. We present research investigating the differential effect of various shoreline protection strategies on recreation. Recreation is a vital contributor to coastal economies and social welfare in coastal communities that relies critically on the condition of the shoreline. 

Visitation is proxied via geotagged social media data from Flickr and Twitter. These are measured as average annual user days, from 2005 - 2017 for Flickr and 2012 - 2017 for Twitter. Values are extracted for cells within a hexagonal tessalation that intersect with the NOAA Environmental Sensitivity Index polyline for the California coastline. The user day counts are treated as a proxy for relative visitation and are the dependent variable in the visitation model.  

*Given that we have count data, and lots of zeros, natural choices for regression are a poisson, negative binomial, zero-inflated negative binomial, or a hurdle model. How to choose between these?*

Zero-inflated doesn't refer to lots of zeros in the outcome variable, but an outcome variable that you would expect to have lots of zeros that are based on a different process than generates non-zero values. As such, even with lots of zeros, a zero inflated negative binomial model may not be appropriate versus a standard NB model if the zeros originate from the same date generating process as the non-zero values. Hurdle models are generally used for sequential decision making and may not be appropriate here, as the decision process to visit the shoreline is typically one step.

Let's first explore some summary statistics. 

```{r data, include=FALSE}
df<-read_csv("results.csv") # Results of "CA-recreation-coastal-hazards.R"
df<-df[complete.cases(df),] # drops a few observations where the precipitation and air temp rasters returned NAs due to inadequate coverage
# rescaling population to thousands and distances to km, to deal with model convergence issues
df$sumpop<-df$sumpop/1000
df$rdist<-df$rdist/1000
df$wtlddist<-df$wtlddist/1000
```

<style>
  td {
    padding: 40px;
  }
</style>

```{r zeros, echo=FALSE, results='asis'}
df$hres<-as.factor(df$hres)
f1<-datasummary((ud == 0) + (ud > 0) ~ (N=1) + Percent(),
            data = df[df$hres==250&df$source=="PUD",],fmt="%.0f", title = 'Flickr Resolution 250m')
f2<-datasummary((ud == 0) + (ud > 0) ~ (N=1) + Percent(),
            data = df[df$hres==500&df$source=="PUD",],fmt="%.0f", title = 'Flickr Resolution 500m')
f3<-datasummary((ud == 0) + (ud > 0) ~ (N=1) + Percent(),
            data = df[df$hres==1000&df$source=="PUD",],fmt="%.0f", title = 'Flickr Resolution 1000m')

knitr::kables(list(f1,f2,f3)) %>%
  kable_styling(position = "center", bootstrap_options = "none")

t1<-datasummary((ud == 0) + (ud > 0) ~ (N=1) + Percent(),
            data = df[df$hres==250&df$source=="TUD",],fmt="%.0f", title = 'Twitter Resolution 250m')
t2<-datasummary((ud == 0) + (ud > 0) ~ (N=1) + Percent(),
            data = df[df$hres==500&df$source=="TUD",],fmt="%.0f", title = 'Twitter Resolution 500m')
t3<-datasummary((ud == 0) + (ud > 0) ~ (N=1) + Percent(),
            data = df[df$hres==1000&df$source=="TUD",],fmt="%.0f", title = 'Twitter Resolution 1000m')

knitr::kables(list(t1,t2,t3)) %>%
  kable_styling(position = "center", bootstrap_options = "none")

rm(f1,f2,f3,t1,t2,t3,summaryname)

```

The data is characterized by a significant number of zeros, that increase as a percentage of the data at higher resolutions. Twitter data seems to produce more zeros than flickr data at all resolutions. 

```{r continuous, echo=FALSE}
datasummary((`Distance to nearest road (km)`=rdist ) + (`Population within 8 miles`=sumpop) + (`Distance to nearest wetland (km)`=wtlddist) + (`Mean annual precipitation (mm/yr)`=meanprec) + (`Mean air temp (C)`=meanat) + (`Mean sea surface temp (C)`=meanSST) ~ Mean + Median + SD + Histogram, data = df[df$hres==250,],fmt="%.2f", title = 'Continuous Predictors 250m')

datasummary((`Distance to nearest road (km)`=rdist ) + (`Population within 8 miles`=sumpop) + (`Distance to nearest wetland (km)`=wtlddist) + (`Mean annual precipitation (mm/yr)`=meanprec) + (`Mean air temp (C)`=meanat) + (`Mean sea surface temp (C)`=meanSST) ~ Mean + Median + SD + Histogram, data = df[df$hres==1000,],fmt="%.2f", title = 'Continuous Predictors 1000m')
```

Summary statistics are shown for all potential continuous predictors. There is little variation across resolutions, but note that the higher resolution observation picks up finer scale variation in the 'distance to X' variables.

```{r discrete, echo=FALSE}
# Filtering area of influence of amenities represented in Yourcoast database
df<-mutate_at(df, vars(BIKE_PATH,BOATING,BT_FACIL_T,CAMPGROUND,DSABLDACSS,FEE,FISHING,PARKING,PTH_BEACH,RESTROOMS,STRS_BEACH), list(~ ifelse(YCdist>1000, 0, .)))

d1<-datasummary((`Exposed rocky shore`=x1a ) + (`Exposed solid man-made structures`=x1b) + (`Exposed wave-cut platforms in bedrock`=x2a) + (`Fine-to-medium grain sand beaches`=x3a) + (`Scarps and steep slopes in sand`=x3b) + (`Coarse-grained sand beaches`=x4) + (`Mixed sand and gravel beaches`=x5) + (`Gravel beaches`=x6a) + (`Riprap`=x6b) + (`Exposed tidal flats`=x7) + (`Sheltered rocky shores`=x8a) + (`Sheltered man-made structures`=x8b) + (`Sheltered riprap`=x8c) + (`Sheltered tidal flat`=x9a) + (`Vegetated low riverine banks`=x9b) + (`Salt and brackish marshes`=x10a) + (`Bike path`=BIKE_PATH) + (`Boating`=BOATING) + (`Boating facilities`=BT_FACIL_T) + (`Campground`=CAMPGROUND) + (`Disabled Access`=DSABLDACSS) + (`Fee`=FEE) + (`Fishing`=FISHING) + (`Parking available`=PARKING) + (`Path to beach`=PTH_BEACH) + (`Restrooms`=RESTROOMS) + (`Stairs to beach`=STRS_BEACH) + (`SF Bay Water Trail`=WaterTrail) + (`SF Bay Trail`=BayTrail) ~ Mean + SD, data = df[df$hres==250,],fmt="%.2f", title = 'Binary Predictors 250m')
d2<-datasummary((`Exposed rocky shore`=x1a ) + (`Exposed solid man-made structures`=x1b) + (`Exposed wave-cut platforms in bedrock`=x2a) + (`Fine-to-medium grain sand beaches`=x3a) + (`Scarps and steep slopes in sand`=x3b) + (`Coarse-grained sand beaches`=x4) + (`Mixed sand and gravel beaches`=x5) + (`Gravel beaches`=x6a) + (`Riprap`=x6b) + (`Exposed tidal flats`=x7) + (`Sheltered rocky shores`=x8a) + (`Sheltered man-made structures`=x8b) + (`Sheltered riprap`=x8c) + (`Sheltered tidal flat`=x9a) + (`Vegetated low riverine banks`=x9b) + (`Salt and brackish marshes`=x10a) + (`Bike path`=BIKE_PATH) + (`Boating`=BOATING) + (`Boating facilities`=BT_FACIL_T) + (`Campground`=CAMPGROUND) + (`Disabled Access`=DSABLDACSS) + (`Fee`=FEE) + (`Fishing`=FISHING) + (`Parking available`=PARKING) + (`Path to beach`=PTH_BEACH) + (`Restrooms`=RESTROOMS) + (`Stairs to beach`=STRS_BEACH) + (`SF Bay Water Trail`=WaterTrail) + (`SF Bay Trail`=BayTrail) ~ Mean + SD, data = df[df$hres==1000,],fmt="%.2f", title = 'Binary Predictors 1000m')

knitr::kables(list(d1,d2)) %>%
  kable_styling(position = "center", bootstrap_options = "none")

d1<-datasummary((`# of ESI types`=types) ~ Mean + SD, data = df[df$hres==250,],fmt="%.2f", title = 'ESI Types 250m')
d2<-datasummary((`# of ESI types`=types) ~ Mean + SD, data = df[df$hres==1000,],fmt="%.2f", title = 'ESI Types 1000m')

knitr::kables(list(d1,d2)) %>%
  kable_styling(position = "center", bootstrap_options = "none")

```

These values represent proportions of observations intersecting with ESI types, or within 1km of amenities in the [California Coastal Commission Yourcoast database](https://www.coastal.ca.gov/YourCoast/#/map). Amenities do not change markedly between resolutions as they are based on intersection with a buffer (within 1km of Yourcoast geotagged location), but there are more ESI types per observation as the resolution gets coarser. 

```{r correlationESI, echo=FALSE, fig.align="center"}
cormat<-round(cor(df[,3:18]),2)
rownames(cormat)<-c("Exposed rocky shore (1a)","Exposed solid man-made structures (1b)","Exposed wave-cut platforms in bedrock (2a)","Fine-to-medium grain sand beaches (3a)","Scarps and steep slopes in sand (3b)","Coarse-grained sand beaches (4)","Mixed sand and gravel beaches (5)","Gravel beaches (6a)","Riprap (6b)","Exposed tidal flats (7)","Sheltered rocky shores (8a)","Sheltered man-made structures (8b)","Sheltered riprap (8c)","Sheltered tidal flat (9a)","Vegetated low riverine banks (9b)","Salt and brackish marshes (10a)")
colnames(cormat)<-c("1a","1b","2a","3a","3b","4","5","6a","6b","7","8a","8b","8c","9a","9b","10a")
corrplot(cormat, method = "color", type = "lower", col = brewer.pal(n=8, name = "RdBu"),tl.col = "black")
```

Correlogram of ESI types. Not much correlation, though tidal flats and marshes are often observed together.

```{r correlationYC, echo=FALSE, fig.align="center"}
cormat<- df[df$hres==1000,] %>%
  dplyr::select(sumpop,rdist,BIKE_PATH,BOATING,BT_FACIL_T,CAMPGROUND,DSABLDACSS,FEE,FISHING,PARKING,PTH_BEACH,RESTROOMS,STRS_BEACH,WaterTrail,BayTrail,wtlddist,meanprec,meanat,meanSST)
cormat<-round(cor(cormat),2)
# rownames(cormat)<-c("Exposed rocky shore (1a)","Exposed solid man-made structures (1b)","Exposed wave-cut platforms in bedrock (2a)","Fine-to-medium grain sand beaches (3a)","Scarps and steep slopes in sand (3b)","Coarse-grained sand beaches (4)","Mixed sand and gravel beaches (5)","Gravel beaches (6a)","Riprap (6b)","Exposed tidal flats (7)","Sheltered rocky shores (8a)","Sheltered man-made structures (8b)","Sheltered riprap (8c)","Sheltered tidal flat (9a)","Vegetated low riverine banks (9b)","Salt and brackish marshes (10a)")
# colnames(cormat)<-c("1a","1b","2a","3a","3b","4","5","6a","6b","7","8a","8b","8c","9a","9b","10a")
par(xpd = TRUE) # https://stackoverflow.com/questions/46331172/corrplot-margin-in-plot-window
corrplot(cormat, method = "color", type = "lower", col = brewer.pal(n=8, name = "RdBu"),tl.col = "black", tl.srt = 45, mar = c(2, 2, 2, 2))

```

Correlogram of other predictors. Relationships are stable across resolutions; the above plots correlations at 1000m resolution.

```{r `ESI crosswalk`, echo=FALSE}

df$RockyShore<-pmax(df$x1a,df$x2a,df$x6a,df$x8a)
df$SandyBeach<-pmax(df$x3a,df$x3b,df$x4,df$x5)
df$Marshes<-pmax(df$x9a,df$x9b,df$x10a)
df$Armored<-pmax(df$x1b,df$x6b,df$x8b,df$x8c)

d1<-datasummary((`Rocky shore`=RockyShore) + (`Sandy beach`=SandyBeach) + (`Marshes`=Marshes) + (`Armored`=Armored) ~ Mean + SD, data = df[df$hres==1000,],fmt="%.2f", title = 'Binary Predictors 1000m')
d2<-datasummary((`Rocky shore`=RockyShore) + (`Sandy beach`=SandyBeach) + (`Marshes`=Marshes) + (`Armored`=Armored) ~ Mean + SD, data = df[df$hres==500,],fmt="%.2f", title = 'Binary Predictors 500m')
d3<-datasummary((`Rocky shore`=RockyShore) + (`Sandy beach`=SandyBeach) + (`Marshes`=Marshes) + (`Armored`=Armored) ~ Mean + SD, data = df[df$hres==250,],fmt="%.2f", title = 'Binary Predictors 250m')

knitr::kables(list(d1,d2,d3)) %>%
  kable_styling(position = "center", bootstrap_options = "none")
```

A proposed crosswalk of ESI types to fewer shoreline classes: Rocky shore {1A, 2A, 6A, 8A}, Sandy beach {3A, 3B, 4, 5}, Marshes {9A, 9B, 10A}, Armored shore {1B, 6B, 8B, 8C}. Again, there are more ESI types per observation as the resolution gets coarser. 


```{r `regressions Poisson`, echo=FALSE}
models<-list()
models[['Flickr 250m']]<-glm(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==250,], family = poisson)
models[['Flickr 500m']]<-glm(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==500,], family = poisson)
models[['Flickr 1000m']]<-glm(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==1000,], family = poisson)
models[['Twitter 250m']]<-glm(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="TUD"&df$hres==250,], family = poisson)
models[['Twitter 500m']]<-glm(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="TUD"&df$hres==500,], family = poisson)
models[['Twitter 1000m']]<-glm(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="TUD"&df$hres==1000,], family = poisson)

msummary(models, stars = TRUE, title = "Poisson regressions, across resolutions and data source")

# coeftest(mod1, vcov = sandwich)

```

```{r `regressions quasiPoisson`, echo=FALSE}
# models <- list()
# models[['Flickr 250m']]<-glm(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==250,], family = quasipoisson)
# models[['Twitter 250m']]<-glm(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="TUD"&df$hres==250,], family = quasipoisson)
# 
# msummary(models, stars = TRUE, title = "Quasi-Poisson regression, across data source at 250m", gof_omit = "AIC|BIC|Log.Lik")
# 
# # coeftest(mod1, vcov = sandwich)
```

```{r `regression NB`, echo=FALSE, results='hide',fig.keep='all'}
models <- list()
models[['Flickr 250m']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==250,], family = nbinom2)
models[['Flickr 500m']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==500,], family = nbinom2)
models[['Flickr 1000m']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==1000,], family = nbinom2)
models[['Twitter 250m']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="TUD"&df$hres==250,], family = nbinom2)
models[['Twitter 500m']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="TUD"&df$hres==500,], family = nbinom2)
models[['Twitter 1000m']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="TUD"&df$hres==1000,], family = nbinom2)


# cm<-c('(Intercept)'='Constant', 'Armored'='Armored','BIKE_PATH'='Bike path','Boating'='BOATING','Boating facilities'='BT_FACIL_T','Campground'='CAMPGROUND','Disabled Access'='DSABLDACSS','Fee'='FEE','Fishing'='FISHING','Parking available'='PARKING','Path to beach'='PTH_BEACH','Restrooms'='RESTROOMS','Stairs to beach'='STRS_BEACH','SF Bay Water Trail'='WaterTrail','SF Bay Trail'='BayTrail')

gm<-modelsummary::gof_map
gm$omit<-FALSE
rows<-data.frame("term"="Num.Obs","Flickr 250m"="27315","Flickr 500m"="12221","Flickr 1000m"="5449","Twitter 250m"="27315","Twitter 500m"="12221","Twitter 1000m"="5449")
attr(rows,"position")<-49

msummary(models, stars = TRUE, title = "Negative binomial regression across data source and resolution", add_rows = rows)

```

```{r `regression comparison flickr`, echo=FALSE}
invisible(glmmTMBControl(optCtrl=list(iter.max=1e4,eval.max=1e3))) 
models <- list()
models[['Poisson']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==250,], family = poisson)
models[['Quasi-Poisson']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==250,], family = nbinom1)
models[['Negative Binomial']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==250,], family = nbinom2)
# Separating the model into two processes for the zeros and the non-zeros
models[['Hurdle']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + rdist + sumpop + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==250,], family = "truncated_nbinom2", zi=~.)
models[['Zero-inflated Negative Binomial']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==250,], zi = ~., family = nbinom2)

gm<-modelsummary::gof_map
gm$omit<-ifelse(gm$raw == "nobs"|gm$raw == "df.residual",TRUE,FALSE)
rows<-data.frame("term"="Num.Obs","Poisson"="27315","Quasi-Poisson"="27315","Negative Binomial"="27315")
attr(rows,"position")<-49

msummary(models[1:3], stars = TRUE, title = "Different regression approaches, Flickr User Days 250m resolution", gof_map = gm, add_rows = rows)

# msummary(models[4], stars = TRUE, title = "Different regression approaches, Flickr User Days 250m resolution", gof_map = gm)
# 
# msummary(models[5], stars = TRUE, title = "Different regression approaches, Flickr User Days 250m resolution", gof_map = gm)
summary(models[[4]])
summary(models[[5]])

```

We have estimated models that treat this as a one (Poisson, Quasi-Poisson, Negative Binomial) or two-stage process (ZINB and hurdle). Based on the log-likelihood and BIC, the ZINB is the best fit.

From a theoretical perspective shoreline visitation choice is not typically treated as a two-stage process in random utility models of visitation. Furthermore, the choice to visit the beach versus do other activities does not seem like it should be well-modeled by the predictors we have here, despite the better fit. Finally, we do not know the home locations or identities of visitors, so we have no reliable way to reconstruct their broader choice set or recover other social/demographic variables.

```{r `Flickr rootograms`, echo=FALSE}
bp.1<-cbind(df[df$source=="PUD"&df$hres==250,"ud"],(as.vector(predict(models[['Poisson']],type = "response"))))
colnames(bp.1)<-c("obs","pred")
bp.1<-pivot_longer(bp.1, cols = c("obs","pred"), names_to = "status", values_to = "ud")
bp.1$status<-as.factor(bp.1$status)
bp.1$ud<-round(bp.1$ud)

ggplot(bp.1) + 
  stat_bin(aes(x=ud, fill=status), position = position_dodge(preserve = "single"), breaks = seq(0, 25, by = 1)) +
  scale_x_continuous(limits = c(0,25)) +
  theme_minimal() +
  ggtitle("Poisson")

bp.2<-cbind(df[df$source=="PUD"&df$hres==250,"ud"],(as.vector(predict(models[['Quasi-Poisson']],type = "response"))))
colnames(bp.2)<-c("obs","pred")
bp.2<-pivot_longer(bp.2, cols = c("obs","pred"), names_to = "status", values_to = "ud")
bp.2$status<-as.factor(bp.2$status)
bp.2$ud<-round(bp.2$ud)

ggplot(bp.2) + 
  stat_bin(aes(x=ud, fill=status), position = position_dodge(preserve = "single"), breaks = seq(0, 25, by = 1)) +
  scale_x_continuous(limits = c(0,25)) +
  theme_minimal() +
  ggtitle("Quasi-Poisson")

bp.3<-cbind(df[df$source=="PUD"&df$hres==250,"ud"],(as.vector(predict(models[['Negative Binomial']],type = "response"))))
colnames(bp.3)<-c("obs","pred")
bp.3<-pivot_longer(bp.3, cols = c("obs","pred"), names_to = "status", values_to = "ud")
bp.3$status<-as.factor(bp.3$status)
bp.3$ud<-round(bp.3$ud)

ggplot(bp.3) + 
  stat_bin(aes(x=ud, fill=status), position = position_dodge(preserve = "single"), breaks = seq(0, 25, by = 1)) +
  scale_x_continuous(limits = c(0,25)) +
  theme_minimal() +
  ggtitle("Negative Binomial")

bp.4<-cbind(df[df$source=="PUD"&df$hres==250,"ud"],(as.vector(predict(models[['Hurdle']],type = "response"))))
colnames(bp.4)<-c("obs","pred")
bp.4<-pivot_longer(bp.4, cols = c("obs","pred"), names_to = "status", values_to = "ud")
bp.4$status<-as.factor(bp.4$status)
bp.4$ud<-round(bp.4$ud)

ggplot(bp.4) + 
  stat_bin(aes(x=ud, fill=status), position = position_dodge(preserve = "single"), breaks = seq(0, 25, by = 1)) +
  scale_x_continuous(limits = c(0,25)) +
  theme_minimal() +
  ggtitle("Hurdle")

bp.5<-cbind(df[df$source=="PUD"&df$hres==250,"ud"],(as.vector(predict(models[['Zero-inflated Negative Binomial']],type = "response"))))
colnames(bp.5)<-c("obs","pred")
bp.5<-pivot_longer(bp.5, cols = c("obs","pred"), names_to = "status", values_to = "ud")
bp.5$status<-as.factor(bp.5$status)
bp.5$ud<-round(bp.5$ud)

ggplot(bp.5) + 
  stat_bin(aes(x=ud, fill=status), position = position_dodge(preserve = "single"), breaks = seq(0, 25, by = 1)) +
  scale_x_continuous(limits = c(0,25)) +
  theme_minimal() +
  ggtitle("Zero-inflated Negative Binomial")
```

Rootograms are a useful way to compare model selection between these two, as they assess [overdispersion](https://www.theanalysisfactor.com/poisson-or-negative-binomial-using-count-model-diagnostics-to-select-a-model/), a key selection criteria between the models. Unfortunately, there is no ready package to summarize the results we have here, so we resort to side-by-side count plots.

The predictive accuracy of the ZINB seems to be qualitatively better than the other models

```{r `regression comparison twitter`, echo=FALSE}
models <- list()
models[['Poisson']]<-glm(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="TUD"&df$hres==250,], family = poisson) # glmmTMB doesn't converge
models[['Quasi-Poisson']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="TUD"&df$hres==250,], family = nbinom1)
models[['Negative Binomial']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="TUD"&df$hres==250,], family = nbinom2)
# Separating the model into two processes for the zeros and the non-zeros does converge, unlike for the Flickr data
models[['Hurdle']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + rdist + sumpop + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="TUD"&df$hres==250,], family = "truncated_nbinom2", zi=~.)
models[['Zero-inflated Negative Binomial']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="TUD"&df$hres==250,], zi = ~., family = nbinom2)

gm<-modelsummary::gof_map
gm$omit<-ifelse(gm$raw == "nobs"|gm$raw == "df.residual"|gm$raw == "deviance"|gm$raw == "df.null"|gm$raw == "sigma"|gm$raw == "null.deviance",TRUE,FALSE)
rows<-data.frame("term"="Num.Obs","Poisson"="27315","Quasi-Poisson"="27315","Negative Binomial"="27315")
attr(rows,"position")<-49

msummary(models[1:3], stars = TRUE, title = "Different regression approaches, Twitter User Days 250m resolution", gof_map = gm, add_rows = rows)

# msummary(models[4], stars = TRUE, title = "Different regression approaches, Twitter User Days 250m resolution", gof_map = gm)
# 
# msummary(models[5], stars = TRUE, title = "Different regression approaches, Twitter User Days 250m resolution", gof_map = gm)

summary(models[[4]])

summary(models[[5]])
```

We have estimated models that treat this as a one (Poisson, Quasi-Poisson, Negative Binomial) or two-stage process (ZINB and hurdle). Based on the log-likelihood and BIC, the hurdle model is the best fit.

```{r `twitter rootograms`, echo=FALSE}

bp.1<-cbind(df[df$source=="TUD"&df$hres==250,"ud"],(as.vector(predict(models[['Poisson']],type = "response"))))
colnames(bp.1)<-c("obs","pred")
bp.1<-pivot_longer(bp.1, cols = c("obs","pred"), names_to = "status", values_to = "ud")
bp.1$status<-as.factor(bp.1$status)
bp.1$ud<-round(bp.1$ud)

ggplot(bp.1) + 
  stat_bin(aes(x=ud, fill=status), position = position_dodge(preserve = "single"), breaks = seq(0, 25, by = 1)) +
  scale_x_continuous(limits = c(0,25)) +
  theme_minimal() +
  ggtitle("Poisson")

bp.2<-cbind(df[df$source=="TUD"&df$hres==250,"ud"],(as.vector(predict(models[['Quasi-Poisson']],type = "response"))))
colnames(bp.2)<-c("obs","pred")
bp.2<-pivot_longer(bp.2, cols = c("obs","pred"), names_to = "status", values_to = "ud")
bp.2$status<-as.factor(bp.2$status)
bp.2$ud<-round(bp.2$ud)

ggplot(bp.2) + 
  stat_bin(aes(x=ud, fill=status), position = position_dodge(preserve = "single"), breaks = seq(0, 25, by = 1)) +
  scale_x_continuous(limits = c(0,25)) +
  theme_minimal() +
  ggtitle("Quasi-Poisson")

bp.3<-cbind(df[df$source=="TUD"&df$hres==250,"ud"],(as.vector(predict(models[['Negative Binomial']],type = "response"))))
colnames(bp.3)<-c("obs","pred")
bp.3<-pivot_longer(bp.3, cols = c("obs","pred"), names_to = "status", values_to = "ud")
bp.3$status<-as.factor(bp.3$status)
bp.3$ud<-round(bp.3$ud)

ggplot(bp.3) + 
  stat_bin(aes(x=ud, fill=status), position = position_dodge(preserve = "single"), breaks = seq(0, 25, by = 1)) +
  scale_x_continuous(limits = c(0,25)) +
  theme_minimal() +
  ggtitle("Negative Binomial")

bp.4<-cbind(df[df$source=="TUD"&df$hres==250,"ud"],(as.vector(predict(models[['Hurdle']],type = "response"))))
colnames(bp.4)<-c("obs","pred")
bp.4<-pivot_longer(bp.4, cols = c("obs","pred"), names_to = "status", values_to = "ud")
bp.4$status<-as.factor(bp.4$status)
bp.4$ud<-round(bp.4$ud)

ggplot(bp.4) + 
  stat_bin(aes(x=ud, fill=status), position = position_dodge(preserve = "single"), breaks = seq(0, 25, by = 1)) +
  scale_x_continuous(limits = c(0,25)) +
  theme_minimal() +
  ggtitle("Hurdle")

bp.5<-cbind(df[df$source=="TUD"&df$hres==250,"ud"],(as.vector(predict(models[['Zero-inflated Negative Binomial']],type = "response"))))
colnames(bp.5)<-c("obs","pred")
bp.5<-pivot_longer(bp.5, cols = c("obs","pred"), names_to = "status", values_to = "ud")
bp.5$status<-as.factor(bp.5$status)
bp.5$ud<-round(bp.5$ud)

ggplot(bp.5) + 
  stat_bin(aes(x=ud, fill=status), position = position_dodge(preserve = "single"), breaks = seq(0, 25, by = 1)) +
  scale_x_continuous(limits = c(0,25)) +
  theme_minimal() +
  ggtitle("Zero-inflated Negative Binomial")

```

The predictive accuracy of the hurdle model seems to be qualitatively better than the other models

```{r `standard errors`, echo=FALSE}
models <- list()
# Need to use pscl package versus glmmTMB for computing heteroskedastic robust standard errors, but there are errors with the variance estimates for the flickr data. Tried various reduced versions using statistical significance from the glmmTMB estimates, did not work. 

write.csv(df,"results2.csv", row.names = FALSE)

# Model does produce robust standard errors in Stata. Code is: 

# import delimited C:\Users\XPSXIII\Downloads\results2.csv
# zinb ud rockyshore sandybeach marshes armored sumpop rdist bike_path boating bt_facil_t campground dsabldacss fee fishing parking pth_beach restrooms strs_beach watertrail baytrail wtlddist meanprec meanat meansst if source=="PUD"&hres==250, inflate(rockyshore sandybeach marshes armored sumpop rdist bike_path boating bt_facil_t campground dsabldacss fee fishing parking pth_beach restrooms strs_beach watertrail baytrail wtlddist meanprec meanat meansst) robust

models[['PUD ZINB']]<-glmmTMB(ud ~ RockyShore + SandyBeach + Marshes + Armored + sumpop + rdist + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="PUD"&df$hres==250,], zi = ~., family = nbinom2)

models[['TUD Hurdle']]<-pscl::hurdle(ud ~ RockyShore + SandyBeach + Marshes + Armored + rdist + sumpop + BIKE_PATH + BOATING + BT_FACIL_T + CAMPGROUND + DSABLDACSS + FEE + FISHING + PARKING + PTH_BEACH + RESTROOMS + STRS_BEACH + WaterTrail + BayTrail + wtlddist + meanprec + meanat + meanSST, data = df[df$source=="TUD"&df$hres==250,], dist = "negbin")

summary(models[['TUD Hurdle']])
```
Hurdle model for TUD 

```{r `BPtestTUD`, echo=FALSE}
# Breusch-Pagan test for heteroskedasticity
bptest(models[['PUD ZINB']]) # p-value <2.2e-16
bptest(models[['TUD Hurdle']])
```
Both models appear to be heteroskedastic according to a Breusch-Pagan test.

```{r `standard errors`, echo=FALSE}
# Heteroskedasticity consistent stanard errors
coeftest(models[['TUD Hurdle']], vcov = sandwich)
```
No approach for robust standard erros in R for ZINB estimated by glmmTMB, so that was exported to Stata to run with robust standard errors

##Draft figures for paper

```{r `map inputs`, echo=FALSE, include=FALSE}
# Guidance for tmaps package https://geocompr.robinlovelace.net/adv-map.html

Rds<-st_read("./Data/Roads_2015.gpkg") # Roads
ESI<-st_read("./Data/ESIL_CA.gpkg") # ESI
bbox<-filter(Rds, FULLNAME %in% c("Pilarcitos Ave")) # Bounding box


PUD500m<-st_read("./Data/PUD_2005-2017_500m.gpkg")
ESI<-st_transform(ESI, st_crs(PUD500m))
bbox<-st_transform(bbox, st_crs(ESI))
PUD500m<-st_join(PUD500m,ESI, left = FALSE)
PUD500m$PUD_YR_AVG<-PUD500m$PUD_YR_AVG*13

TUD500m<-st_read("./Data/TUD_2012-2017_500m.gpkg")
TUD500m<-st_join(TUD500m,ESI, left = FALSE)
TUD500m$PUD_YR_AVG<-TUD500m$PUD_YR_AVG*6

# PUD250m<-st_read("./Data/PUD_2005-2017_250m.gpkg")
# PUD250m<-st_join(PUD250m,ESI, left = FALSE)
# PUD250m$PUD_YR_AVG<-PUD250m$PUD_YR_AVG*13 # Total PUD vs annual average
# 
# TUD250m<-st_read("./Data/TUD_2012-2017_250m.gpkg")
# TUD250m<-st_join(TUD250m,ESI, left = FALSE)
# TUD250m$PUD_YR_AVG<-TUD250m$PUD_YR_AVG*6 # Total TUD vs annual average
# 
# PUD1k<-st_read("./Data/PUD_2005-2017_1000m.gpkg")
# PUD1k<-st_join(PUD1k,ESI, left = FALSE)
# PUD1k$PUD_YR_AVG<-PUD1k$PUD_YR_AVG*13
# 
# TUD1k<-st_read("./Data/TUD_2012-2017_1000m.gpkg")
# TUD1k<-st_join(TUD1k,ESI, left = FALSE)
# TUD1k$PUD_YR_AVG<-TUD1k$PUD_YR_AVG*6

# Modifying the bounding box https://www.jla-data.net/eng/adjusting-bounding-box-of-a-tmap-map/
bbox_new<-st_bbox(bbox)
xrange <- bbox_new$xmax - bbox_new$xmin
yrange <- bbox_new$ymax - bbox_new$ymin
  bbox_new[1] <- bbox_new[1] - (8 * xrange) # xmin - left
  bbox_new[3] <- bbox_new[3] + (4 * xrange) # xmax - right
  bbox_new[2] <- bbox_new[2] - (6 * yrange) # ymin - bottom
  bbox_new[4] <- bbox_new[4] + (6 * yrange) # ymax - top
bbox_new <- bbox_new %>%  # take the bounding box ...
  st_as_sfc() # ... and make it a sf polygon

```

```{r `maps`, echo=FALSE}
cdp<-places("CA", cb = TRUE) # Place names from US Census (tigris package)
cdp<-st_transform(cdp, st_crs(ESI))
rd.sc<-roads("CA","San Mateo") # Roads from US Census in San Mateo County, CA
rd.sc<-st_transform(rd.sc, st_crs(ESI))

breaks = c(0, 100, 200, 300, 400)
PUD500m.p<-tm_shape(rd.sc,bbox = bbox_new) +
  tm_lines(col="grey") +
  tm_shape(PUD500m) +
  tm_polygons(col = "PUD_YR_AVG", breaks = breaks, title = "Flickr User Days") + 
  tm_shape(ESI) +
  tm_lines(col="black") +
  tm_shape(cdp) +
  tm_text("NAME",size=0.75) +
  tm_add_legend("line", col = "black", labels = "ESI Line")

breaks = c(0, 200, 400, 600, 800)
TUD500m.p<-tm_shape(rd.sc,bbox = bbox_new) +
  tm_lines(col="grey") +
  tm_shape(TUD500m) +
  tm_polygons(col = "PUD_YR_AVG", breaks = breaks, title = "Twitter User Days") + 
  tm_shape(ESI) +
  tm_lines(col="black") +
  tm_shape(cdp) +
  tm_text("NAME",size=0.75) +
  tm_add_legend("line", col = "black", labels = "ESI Line")

plots500m<-tmap_arrange(PUD500m.p,TUD500m.p,ncol = 2)

plots500m

```
